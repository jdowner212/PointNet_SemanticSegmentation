{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create variables with paths to various folders in the repository:\n",
    "path_to_repository_base = ''\n",
    "path_to_sem_seg = ''\n",
    "path_to_utils = ''\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from   plotly.colors import n_colors\n",
    "from   plotly.subplots import make_subplots\n",
    "import math\n",
    "import h5py\n",
    "import socket\n",
    "import shutil\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from   numpy.random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import threadpoolctl\n",
    "import scipy\n",
    "import joblib\n",
    "import cmake\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "!pip install open3d path trimesh\n",
    "import path\n",
    "import open3d as o3d\n",
    "from open3d import utility as utility\n",
    "V3dV =  o3d.utility.Vector3dVector\n",
    "import torch\n",
    "import MyCloud_utils\n",
    "from MyCloud_utils import *\n",
    "sys.path.append(path_to_repository)\n",
    "sys.path.append(path_to_sem_seg)\n",
    "sys.path.append(path_to_utils)\n",
    "import provider as provider\n",
    "import tf_util as tf_util\n",
    "from model import *\n",
    "import time\n",
    "import IPython\n",
    "from IPython.display import Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "######## TRAIN ########\n",
    "#######################\n",
    "\n",
    "'''\n",
    "I had issues running train.py from colab. My solution was to run the commented-out code below directly within the notebook.\n",
    "'''\n",
    "\n",
    "# os.chdir(path_to_sem_seg)\n",
    "# BASE_DIR = path_to_sem_seg\n",
    "# ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "# sys.path.append(BASE_DIR)\n",
    "# sys.path.append(ROOT_DIR)\n",
    "# sys.path.append(os.path.join(ROOT_DIR, 'utils')) #sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "# import provider as provider\n",
    "# import tf_util as tf_util\n",
    "# from model import *\n",
    "\n",
    "\n",
    "# #!python train.py --log_dir log6 --test_area 6\n",
    "# # 8192\n",
    "# FLAGS = {'gpu': 0, 'log_dir': 'log3', 'num_point' : 4096, 'max_epoch': 100, 'batch_size': 36, 'learning_rate': 0.001, 'momentum': 0.9, 'optimizer': 'adam', 'decay_step': 200000, 'decay_rate': 0.7, 'test_area': 3}\n",
    "\n",
    "# BATCH_SIZE = FLAGS['batch_size']\n",
    "# NUM_POINT = FLAGS['num_point']\n",
    "# MAX_EPOCH = FLAGS['max_epoch']\n",
    "# NUM_POINT = FLAGS['num_point']\n",
    "# BASE_LEARNING_RATE = FLAGS['learning_rate']\n",
    "# GPU_INDEX = FLAGS['gpu']\n",
    "# MOMENTUM = FLAGS['momentum']\n",
    "# OPTIMIZER = FLAGS['optimizer']\n",
    "# DECAY_STEP = FLAGS['decay_step']\n",
    "# DECAY_RATE = FLAGS['decay_rate']\n",
    "# LOG_DIR = FLAGS['log_dir']\n",
    "\n",
    "# if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "# os.system('cp model.py %s' % (LOG_DIR)) # bkp of model def\n",
    "# os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "# LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "# MAX_NUM_POINT = NUM_POINT\n",
    "# NUM_CLASSES = 13\n",
    "\n",
    "# BN_INIT_DECAY = 0.5\n",
    "# BN_DECAY_DECAY_RATE = 0.5\n",
    "# #BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
    "# BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "# BN_DECAY_CLIP = 0.99\n",
    "\n",
    "# HOSTNAME = socket.gethostname()\n",
    "\n",
    "\n",
    "# ALL_FILES = provider.getDataFiles(ROOT_DIR + '/data/indoor3d_sem_seg_hdf5_data/all_files.txt')\n",
    "# room_filelist = [line.rstrip() for line in open(ROOT_DIR + '/data/indoor3d_sem_seg_hdf5_data/room_filelist.txt')]\n",
    "\n",
    "# # Load ALL data\n",
    "# data_batch_list = []\n",
    "# label_batch_list = []\n",
    "# for h5_filename in ALL_FILES:\n",
    "#     data_batch, label_batch = provider.loadDataFile(ROOT_DIR + '/data/' + h5_filename)\n",
    "#     data_batch_list.append(data_batch)\n",
    "#     label_batch_list.append(label_batch)\n",
    "# data_batches = np.concatenate(data_batch_list, 0)\n",
    "# label_batches = np.concatenate(label_batch_list, 0)\n",
    "# print(data_batches.shape)\n",
    "# print(label_batches.shape)\n",
    "\n",
    "# test_area = 'Area_'+str(FLAGS['test_area'])\n",
    "# train_idxs = []\n",
    "# test_idxs = []\n",
    "# for i,room_name in enumerate(room_filelist):\n",
    "#     if test_area in room_name:\n",
    "#         test_idxs.append(i)\n",
    "#     else:\n",
    "#         train_idxs.append(i)\n",
    "\n",
    "# train_data = data_batches[train_idxs,...]\n",
    "# train_label = label_batches[train_idxs]\n",
    "# test_data = data_batches[test_idxs,...]\n",
    "# test_label = label_batches[test_idxs]\n",
    "# print(train_data.shape, train_label.shape)\n",
    "# print(test_data.shape, test_label.shape)\n",
    "\n",
    "# def log_string(out_str):\n",
    "#     LOG_FOUT.write(out_str+'\\n')\n",
    "#     LOG_FOUT.flush()\n",
    "#     print(out_str)\n",
    "\n",
    "# def get_learning_rate(batch):\n",
    "#     learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "#                         BASE_LEARNING_RATE,  # Base learning rate.\n",
    "#                         batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "#                         DECAY_STEP,          # Decay step.\n",
    "#                         DECAY_RATE,          # Decay rate.\n",
    "#                         staircase=True)\n",
    "#     learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "#     return learning_rate        \n",
    "\n",
    "# def get_bn_decay(batch):\n",
    "#     bn_momentum = tf.compat.v1.train.exponential_decay(\n",
    "#                       BN_INIT_DECAY, batch*BATCH_SIZE, BN_DECAY_DECAY_STEP,BN_DECAY_DECAY_RATE, staircase=True)\n",
    "#     bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "#     return bn_decay\n",
    "\n",
    "# def train():\n",
    "#     with tf.Graph().as_default():\n",
    "#         with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "#             pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "#             is_training_pl = tf.compat.v1.placeholder(tf.bool, shape=())\n",
    "            \n",
    "#             # Note the global_step=batch parameter to minimize. \n",
    "#             # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "#             batch = tf.Variable(0)\n",
    "#             bn_decay = get_bn_decay(batch)\n",
    "#             tf.compat.v1.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "#             # Get model and loss \n",
    "#             pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "#             loss = get_loss(pred, labels_pl)\n",
    "#             tf.compat.v1.summary.scalar('loss', loss)\n",
    "\n",
    "#             correct = tf.equal(tf.argmax(input=pred, axis=2), tf.cast(labels_pl, dtype=tf.int64))\n",
    "#             accuracy = tf.reduce_sum(input_tensor=tf.cast(correct, tf.float32)) / float(BATCH_SIZE*NUM_POINT)\n",
    "#             tf.compat.v1.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "#             # Get training operator\n",
    "#             learning_rate = get_learning_rate(batch)\n",
    "#             tf.compat.v1.summary.scalar('learning_rate', learning_rate)\n",
    "#             if OPTIMIZER == 'momentum':\n",
    "#                 optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "#             elif OPTIMIZER == 'adam':\n",
    "#                 optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "#             train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "#             # Add ops to save and restore all the variables.\n",
    "#             saver = tf.compat.v1.train.Saver()\n",
    "            \n",
    "#         # Create a session\n",
    "#         config = tf.compat.v1.ConfigProto()\n",
    "#         config.gpu_options.allow_growth = True\n",
    "#         config.allow_soft_placement = True\n",
    "#         config.log_device_placement = True\n",
    "#         sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "#         # Add summary writers\n",
    "#         merged = tf.compat.v1.summary.merge_all()\n",
    "#         train_writer = tf.compat.v1.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "#                                   sess.graph)\n",
    "#         test_writer = tf.compat.v1.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "#         # Init variables\n",
    "#         init = tf.compat.v1.global_variables_initializer()\n",
    "#         sess.run(init, {is_training_pl:True})\n",
    "\n",
    "#         ops = {'pointclouds_pl': pointclouds_pl,\n",
    "#                'labels_pl': labels_pl,\n",
    "#                'is_training_pl': is_training_pl,\n",
    "#                'pred': pred,\n",
    "#                'loss': loss,\n",
    "#                'train_op': train_op,\n",
    "#                'merged': merged,\n",
    "#                'step': batch}\n",
    "\n",
    "#         for epoch in range(MAX_EPOCH):\n",
    "#             now = time.time()\n",
    "#             log_string('**** EPOCH %03d ****' % (epoch))\n",
    "#             sys.stdout.flush()\n",
    "             \n",
    "#             train_one_epoch(sess, ops, train_writer)\n",
    "#             eval_one_epoch(sess, ops, test_writer)\n",
    "            \n",
    "#             # Save the variables to disk.\n",
    "#             if epoch % 10 == 0:\n",
    "#                 save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "#                 log_string(\"Model saved in file: %s\" % save_path)\n",
    "#             print('Total time spent int epoch: {}'.format(np.round(time.time()-now,3)))\n",
    "\n",
    "\n",
    "# def train_one_epoch(sess, ops, train_writer):\n",
    "#     \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "#     is_training = True\n",
    "    \n",
    "#     log_string('----')\n",
    "#     current_data, current_label, _ = provider.shuffle_data(train_data[:,0:NUM_POINT,:], train_label) \n",
    "    \n",
    "#     file_size = current_data.shape[0]\n",
    "#     num_batches = file_size // BATCH_SIZE\n",
    "    \n",
    "#     total_correct = 0\n",
    "#     total_seen = 0\n",
    "#     loss_sum = 0\n",
    "    \n",
    "#     for batch_idx in range(num_batches):\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "#         start_idx = batch_idx * BATCH_SIZE\n",
    "#         end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "#         feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "#                      ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "#                      ops['is_training_pl']: is_training,}\n",
    "#         summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred']],\n",
    "#                                          feed_dict=feed_dict)\n",
    "#         train_writer.add_summary(summary, step)\n",
    "#         pred_val = np.argmax(pred_val, 2)\n",
    "#         correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "#         total_correct += correct\n",
    "#         total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "#         loss_sum += loss_val\n",
    "    \n",
    "#     log_string('mean loss:  %f' % (loss_sum / float(num_batches)))\n",
    "#     log_string('accuracy:   %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        \n",
    "# def eval_one_epoch(sess, ops, test_writer):\n",
    "#     \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "#     is_training = False\n",
    "#     total_correct = 0\n",
    "#     total_seen = 0\n",
    "#     loss_sum = 0\n",
    "#     total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "#     total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "#     log_string('----')\n",
    "#     current_data = test_data[:,0:NUM_POINT,:]\n",
    "#     current_label = np.squeeze(test_label)\n",
    "    \n",
    "#     file_size = current_data.shape[0]\n",
    "#     num_batches = file_size // BATCH_SIZE\n",
    "    \n",
    "#     for batch_idx in range(num_batches):\n",
    "#         start_idx = batch_idx * BATCH_SIZE\n",
    "#         end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "#         feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "#                      ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "#                      ops['is_training_pl']: is_training}\n",
    "#         summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred']],\n",
    "#                                       feed_dict=feed_dict)\n",
    "#         test_writer.add_summary(summary, step)\n",
    "#         pred_val = np.argmax(pred_val, 2)\n",
    "#         correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "#         total_correct += correct\n",
    "#         total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "#         loss_sum += (loss_val*BATCH_SIZE)\n",
    "#         for i in range(start_idx, end_idx):\n",
    "#             for j in range(NUM_POINT):\n",
    "#                 l = current_label[i, j]\n",
    "#                 total_seen_class[l] += 1\n",
    "#                 total_correct_class[l] += (pred_val[i-start_idx, j] == l)\n",
    "            \n",
    "#     log_string('eval mean loss:     %f' % (loss_sum / float(total_seen/NUM_POINT)))\n",
    "#     log_string('eval accuracy:      %f' % (total_correct / float(total_seen)))\n",
    "#     log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train()\n",
    "#     LOG_FOUT.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
